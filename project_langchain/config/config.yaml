# LangChain RAG Pipeline Configuration

# Model settings
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"
  device: "cuda"  # "cuda" or "cpu"
  max_new_tokens: 200
  temperature: 0.1
  top_p: 0.9
  use_compile: true  # Use torch.compile for faster inference

# Retrieval settings
retrieval:
  top_k: 3  # Number of documents to retrieve
  embedding_model: "all-MiniLM-L6-v2"  # SentenceTransformer model

# Vector store settings
vector_store:
  collection_name: "chalmers_courses"
  persist_directory: "data/chroma_db"
  embedding_function: "huggingface"  # Options: "huggingface"

# UI settings
ui:
  title: "Chalmers Course RAG Chatbot"
  description: "Ask questions about Chalmers courses using RAG"
  port: 7860
  share: false  # Set to true for public Gradio link
  show_sources: true
  enable_chat_history: false  # Set to true for conversation memory

# Data paths
data:
  courses_file: "data/processed/courses_clean.json"
  raw_data_dir: "data/raw"
  processed_data_dir: "data/processed"

