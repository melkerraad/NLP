# LangChain RAG Pipeline Configuration

# Model settings
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"
  device: "cuda"  # "cuda" or "cpu"
  max_new_tokens: 200
  temperature: 0.1
  top_p: 0.9
  use_compile: true  # Use torch.compile for faster inference

# Retrieval settings
retrieval:
  top_k: 3  # Default number of documents (used as base for dynamic K)
  max_k: 30  # Maximum documents for list queries
  min_k: 1   # Minimum documents
  embedding_model: "all-MiniLM-L6-v2"  # SentenceTransformer model
  # Dynamic K settings:
  # - Specific course queries (e.g., "Is DAT450 compulsory?"): k=1
  # - Comparison queries: k=5-10
  # - List queries (e.g., "all compulsory courses"): k=20
  # - General queries: k=3 (default)

# Vector store settings
vector_store:
  collection_name: "chalmers_courses"
  persist_directory: "data/chroma_db"
  embedding_function: "huggingface"  # Options: "huggingface"

# UI settings
ui:
  title: "Chalmers Course RAG Chatbot"
  description: "Ask questions about Chalmers courses using RAG"
  port: 7860
  share: false  # Set to true for public Gradio link
  show_sources: true
  enable_chat_history: false  # Set to true for conversation memory

# Data paths
data:
  courses_file: "data/processed/courses_clean.json"
  raw_data_dir: "data/raw"
  processed_data_dir: "data/processed"

